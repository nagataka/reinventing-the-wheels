{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chap. 2\n",
    "\n",
    "[WordNet](https://wordnet.princeton.edu/)\n",
    "\n",
    "## カウントベースの手法\n",
    "\n",
    "分布仮説（Distributional Hypothesis）: The Distributional Hypothesis is that words that occur in the same contexts tend to have similar meanings (Harris, 1954)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[query] you\n",
      "goodbye: 0.7071067691154799\n",
      "i: 0.7071067691154799\n",
      "hello: 0.7071067691154799\n",
      "say: 0.0\n",
      "and: 0.0\n",
      "[[0 1 0 0 0 0 0]\n",
      " [1 0 1 0 1 1 0]\n",
      " [0 1 0 1 0 0 0]\n",
      " [0 0 1 0 1 0 0]\n",
      " [0 1 0 1 0 0 0]\n",
      " [0 1 0 0 0 0 1]\n",
      " [0 0 0 0 0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def preprocess(text):\n",
    "    \"\"\"Create a word list and corresponding index list\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    text = text.replace('.', ' .')\n",
    "    words = text.split(' ')\n",
    "\n",
    "    word_to_id = {}\n",
    "    id_to_word = {}\n",
    "\n",
    "    for word in words:\n",
    "        if word not in word_to_id:\n",
    "            new_id = len(word_to_id)\n",
    "            word_to_id[word] = new_id\n",
    "            id_to_word[new_id] = word\n",
    "        \n",
    "        #print(word_to_id)\n",
    "        #print(id_to_word)\n",
    "\n",
    "    # create a corpus\n",
    "    corpus = [word_to_id[w] for w in words]\n",
    "    corpus = np.array(corpus)\n",
    "    \n",
    "    return corpus, word_to_id, id_to_word\n",
    "\n",
    "def create_co_matrix(corpus, vocab_size, window_size=1):\n",
    "    corpus_size = len(corpus)\n",
    "    co_matrix = np.zeros((vocab_size, vocab_size), dtype=np.int32)\n",
    "    \n",
    "    for idx, word_id in enumerate(corpus):\n",
    "        for i in range(1, window_size + 1):\n",
    "            left_idx = idx-1\n",
    "            right_idx = idx+1\n",
    "            \n",
    "            if left_idx >= 0:\n",
    "                left_word_id = corpus[left_idx]\n",
    "                co_matrix[word_id, left_word_id] += 1\n",
    "                \n",
    "            if right_idx < corpus_size:\n",
    "                right_word_id = corpus[right_idx]\n",
    "                co_matrix[word_id, right_word_id] += 1\n",
    "                \n",
    "    return co_matrix\n",
    "\n",
    "def cos_similarity(x, y, eps=1e-8):\n",
    "    nx = x / (np.sqrt(np.sum(x**2)) + eps)\n",
    "    ny = y / (np.sqrt(np.sum(y**2)) + eps)\n",
    "    return np.dot(nx, ny)\n",
    "\n",
    "def most_similar(query, word_to_id, id_to_word, word_matrix, top=5):\n",
    "    if query not in word_to_id:\n",
    "        print(\"%s is not found\" % query)\n",
    "        return\n",
    "    \n",
    "    print('\\n[query] '+query)\n",
    "    query_id = word_to_id[query]\n",
    "    query_vec = word_matrix[query_id]\n",
    "    \n",
    "    vocab_size = len(id_to_word)\n",
    "    similarity = np.zeros(vocab_size)\n",
    "    for i in range(vocab_size):\n",
    "        similarity[i] = cos_similarity(word_matrix[i], query_vec)\n",
    "        \n",
    "    count = 0\n",
    "    for i in (-1 * similarity).argsort():\n",
    "        if id_to_word[i] == query:\n",
    "            continue\n",
    "        print('%s: %s' % (id_to_word[i], similarity[i]))\n",
    "        count += 1\n",
    "        if count >= top:\n",
    "            return\n",
    "        \n",
    "text = \"You say goodbye and I say hello.\"\n",
    "corpus, word_to_id, id_to_word = preprocess(text)\n",
    "C = create_co_matrix(corpus, len(word_to_id))\n",
    "\n",
    "c0 = C[word_to_id['you']]\n",
    "c1 = C[word_to_id['i']]\n",
    "#print(cos_similarity(c0, c1))\n",
    "most_similar('you', word_to_id, id_to_word, C)\n",
    "print(C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Improve count base method\n",
    "\n",
    "* [(Positive) Pointwise mutual information](https://en.wikipedia.org/wiki/Pointwise_mutual_information)\n",
    "* [SVD](https://en.wikipedia.org/wiki/Singular_value_decomposition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ppmi(C, verbose=False, eps=1e-8):\n",
    "    M = np.zeros_like(C, dtype=np.float32)\n",
    "    N = np.sum(C)\n",
    "    S = np.sum(C, axis=0)\n",
    "    total = C.shape[0] * C.shape[1]\n",
    "    cnt = 0\n",
    "    \n",
    "    for i in range(C.shape[0]):\n",
    "        for j in range(C.shape[1]):\n",
    "            pmi = np.log2(C[i, j] * N / (S[j]*S[i]) + eps)\n",
    "            M[i, j] = max(0, pmi)\n",
    "            \n",
    "            if verbose:\n",
    "                cnt += 1\n",
    "                if cnt % (total//100) == 0:\n",
    "                    print('%.1f%% done' % (100*cnt/total))\n",
    "                    \n",
    "    return M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "covariance matrix\n",
      "[[0 1 0 0 0 0 0]\n",
      " [1 0 1 0 1 1 0]\n",
      " [0 1 0 1 0 0 0]\n",
      " [0 0 1 0 1 0 0]\n",
      " [0 1 0 1 0 0 0]\n",
      " [0 1 0 0 0 0 1]\n",
      " [0 0 0 0 0 1 0]]\n",
      "--------------------------------------------------\n",
      "PPMI\n",
      "[[0.    1.807 0.    0.    0.    0.    0.   ]\n",
      " [1.807 0.    0.807 0.    0.807 0.807 0.   ]\n",
      " [0.    0.807 0.    1.807 0.    0.    0.   ]\n",
      " [0.    0.    1.807 0.    1.807 0.    0.   ]\n",
      " [0.    0.807 0.    1.807 0.    0.    0.   ]\n",
      " [0.    0.807 0.    0.    0.    0.    2.807]\n",
      " [0.    0.    0.    0.    0.    2.807 0.   ]]\n",
      "--------------------------------------------------\n",
      "U (SVD)\n",
      "[[-3.409e-01 -1.110e-16 -3.886e-16 -1.205e-01  0.000e+00  9.323e-01\n",
      "   2.226e-16]\n",
      " [ 0.000e+00 -5.976e-01  1.802e-01  0.000e+00 -7.812e-01  0.000e+00\n",
      "   0.000e+00]\n",
      " [-4.363e-01 -4.241e-17 -2.172e-16 -5.088e-01 -1.767e-17 -2.253e-01\n",
      "  -7.071e-01]\n",
      " [-2.614e-16 -4.978e-01  6.804e-01 -6.574e-17  5.378e-01  9.951e-17\n",
      "   1.201e-17]\n",
      " [-4.363e-01 -3.229e-17 -1.654e-16 -5.088e-01 -1.345e-17 -2.253e-01\n",
      "   7.071e-01]\n",
      " [-7.092e-01 -3.229e-17 -1.654e-16  6.839e-01 -1.345e-17 -1.710e-01\n",
      "   5.889e-17]\n",
      " [ 3.056e-16 -6.285e-01 -7.103e-01  8.846e-17  3.169e-01 -2.847e-16\n",
      "  -1.546e-17]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "text = 'You say goodbye and I say hello.'\n",
    "corpus, word_to_id, id_to_word = preprocess(text)\n",
    "vocab_size = len(word_to_id)\n",
    "C = create_co_matrix(corpus, vocab_size)\n",
    "W = ppmi(C)\n",
    "\n",
    "# SVD\n",
    "U, S, V = np.linalg.svd(W)\n",
    "\n",
    "np.set_printoptions(precision=3)\n",
    "print('covariance matrix')\n",
    "print(C)\n",
    "print('-'*50)\n",
    "print('PPMI')\n",
    "print(W)\n",
    "print('-'*50)\n",
    "print('U (SVD)')\n",
    "print(U)\n",
    "\n",
    "for word, word_id in word_to_id.items():\n",
    "    plt.annotate(word, (U[word_id, 0], U[word_id, 1]))\n",
    "    \n",
    "plt.scatter(U[:,0], U[:,1], alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chap. 3\n",
    "\n",
    "##### カウントベースの手法の問題点\n",
    "\n",
    "コーパスで扱う語彙数が増えた際に、共起行列が巨大になることに加え、SVDは計算量が $O(n^3)$ なので現実的でなくなる。\n",
    "\n",
    "## word2vec\n",
    "\n",
    "* CBOW (Continuous bag-of-words)\n",
    "    - コンテキストからターゲットを推測することを目的としたニューラルネットワーク\n",
    "    - 「ターゲット」は中央の単語、その周囲の単語が「コンテキスト」\n",
    "* skip-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class MatMul:\n",
    "    def __init__(self, W):\n",
    "        self.params = [W]\n",
    "        self.grads = [np.zeros_like(W)]\n",
    "        self.x = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        W, = self.params\n",
    "        out = np.dot(x, W)\n",
    "        self.x = x\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        W, = self.params\n",
    "        dx = np.dot(dout, W.T)\n",
    "        dW = np.dot(self.x.T, dout)\n",
    "        self.grads[0][...] = dW\n",
    "        return dx\n",
    "    \n",
    "def convert_one_hot(target, vocab_size):\n",
    "    new_shape = (*target.shape, vocab_size)\n",
    "    one_hot = np.zeros(new_shape, dtype=int)\n",
    "    return one_hot\n",
    "\n",
    "def create_contexts_target(corpus, window_size=1):\n",
    "    target = corpus[window_size:-window_size]\n",
    "    contexts = []\n",
    "    \n",
    "    for idx in range(window_size, len(corpus)-window_size):\n",
    "        cs = []\n",
    "        for t in range(-window_size, window_size + 1):\n",
    "            if t == 0:\n",
    "                continue\n",
    "            cs.append(corpus[idx + t])\n",
    "        contexts.append(cs)\n",
    "        \n",
    "    return np.array(contexts), np.array(target)\n",
    "\n",
    "contexts, target = create_contexts_target(corpus, window_size=1)\n",
    "one_hot = convert_one_hot(target, 7)\n",
    "print(one_hot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chap. 5 RNN\n",
    "\n",
    "合わせて [Deep Learnng Book](http://www.deeplearningbook.org/contents/rnn.html) も。\n",
    "\n",
    "##### 言語モデル\n",
    "単語の並びに対して確率を与える。「それがどれだけ起こり得るのか（それがどれだけ自然な単語の並びであるのか）」という事を確率で評価する。\n",
    "\n",
    "### RNN とは\n",
    "\n",
    "順伝播は以下の式で表される。\n",
    "\n",
    "$h_t = tanh(h_{t-1}W_h + x_tW_x + b)$\n",
    "\n",
    "実装は RNN クラスを参照。改めて見てみると、言ってしまえばただこれだけの事。\n",
    "\n",
    "\n",
    "「RNNは、コンテキストがどれだけ長くても、そのコンテキストの情報を記憶するメカニズムを持つ」というのは、前のステップからの入力 $h_(t-1)$ にそれまでのコンテキストが含まれたマルコフ性の事をこう言っているのかな。\n",
    "\n",
    "RNNを図示する際、ループ構造を展開して、各時刻$t$を一つ一つのレイヤーとみなす表記が一般的に使われる。\n",
    "\n",
    "##### Backpropagation Through Time (BPTT)\n",
    "「時間方向に展開したニューラルネットワークの誤差逆伝播法」\n",
    "\n",
    "※時系列データの時間サイズが大きくなるに比例して、BPTTで消費するコンピュータの計算リソースが増加する事と、逆伝播時の勾配が不安定になる事が問題になるため、（逆伝播時の）ネットワークのつながりを適当な長さで断ち切り、小さなネットワークを複数作るというアイデア、Truncated BPTT が実用ではよく行われる。なお、正しくは、ネットワークの「逆伝播」のつながりだけを断ち切る事になる。順伝播のつながりは維持したまま、逆伝播のつながりは適当な長さで断ち切り、その切り取られたネットワーク単位で学習を行う。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN:\n",
    "    def __init__(self, Wx, Wh, b):\n",
    "        self.params = [Wx, Wh, b]\n",
    "        self.grads = [np.zeros_like(Wx), np.zeros_like(Wh), np.zeros_like(b)]\n",
    "        self.cache = None\n",
    "        \n",
    "    def forward(self, x, h_prev):\n",
    "        Wx, Wh, b = self.params\n",
    "        t = np.dot(h_prev, Wh) + np.dot(x, Wx) + b\n",
    "        h_next = np.tanh(t)\n",
    "        \n",
    "        self.cache = (x, h_prev, h_next)\n",
    "        return h_next\n",
    "    \n",
    "    def backward(self, dh_next):\n",
    "        Wx, Wh, b = self.params\n",
    "        x, h_prev, h_next = self.cache\n",
    "        \n",
    "        dt = dh_next * (1 - h_next ** 2)\n",
    "        db = np.sum(dt, axis=0)\n",
    "        dWh = np.dot(h_prev.T, dt)\n",
    "        dh_prev = np.dot(dt, Wh.T)\n",
    "        dWx = np.dot(x.T, dt)\n",
    "        dx = np.dot(dt, Wx.T)\n",
    "        \n",
    "        self.grads[0][...] = dWx\n",
    "        self.grads[1][...] = dWh\n",
    "        self.grads[2][...] = db\n",
    "        \n",
    "        return dx, dh_prev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "再掲：順伝播は以下の式で表される。\n",
    "\n",
    "$h_t = tanh(h_{t-1}W_h + x_tW_x + b)$\n",
    "\n",
    "実装は RNN クラスを参照。改めて見てみると、言ってしまえばただこれだけの事。\n",
    "\n",
    "```python\n",
    "t = np.dot(h_prev, Wh) + np.dot(x, Wx) + b\n",
    "h_next = np.tanh(t)\n",
    "```\n",
    "\n",
    "これを連ねるだけ。また、cacheは逆伝播の計算時に使用する中間データを保持するための変数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeRNN:\n",
    "    def __init__(self, Wx, Wh, b, stateful=False):\n",
    "        self.params = [Wx, Wh, b]\n",
    "        self.grads = [np.zeros_like(Wx), np.zeros_like(Wh), np.zeros_like(b)]\n",
    "        self.layers = None\n",
    "        \n",
    "        self.h, self.dh = None, None\n",
    "        self.stateful = stateful\n",
    "        \n",
    "    def set_state(self, h):\n",
    "        self.h = h\n",
    "        \n",
    "    def reset_state(self):\n",
    "        self.h = None\n",
    "        \n",
    "    def forward(self, xs):\n",
    "        \"\"\"\n",
    "        入力xsを受け取る。xsは、T個文の時系列データを一つにまとめたもの。\n",
    "        N: バッチサイズ\n",
    "        T: タイムステップ数\n",
    "        D: 入力ベクトルの次元数\n",
    "        H: 隠れ層の次元数\n",
    "        \n",
    "        self.h = layer.forward(xs[:, t, :], self.h) については、p.194参照\n",
    "        各バッチの t から T+t 番目のデータを用いているという事か\n",
    "        \"\"\"\n",
    "        Wx, Wh, b = self.params\n",
    "        N, T, D = xs.shape\n",
    "        D, H = Wx.shape\n",
    "        \n",
    "        self.layers = []\n",
    "        hs = np.empty((N, T, H), dtype='f')\n",
    "        \n",
    "        if not self.stateful or self.h is None:\n",
    "            self.h = np.zeros((N, H), dtype='f')\n",
    "            \n",
    "        for t in range(T):\n",
    "            layer = RNN(*self.params)\n",
    "            self.h = layer.forward(xs[:, t, :], self.h)\n",
    "            hs[:, t, :] = self.h\n",
    "            self.layers.append(layer)\n",
    "            \n",
    "        return hs\n",
    "    \n",
    "    def backward(self, dhs):\n",
    "        Wx, Wh, b = self.params\n",
    "        N, T, H = dhs.shape\n",
    "        D, H = Wx.shape\n",
    "        \n",
    "        dxs = np.empty((N, T, D), dtype='f')\n",
    "        dh = 0\n",
    "        grads = [0, 0, 0]\n",
    "        for t in reversed(range(T)):\n",
    "            layer = self.layers[t]\n",
    "            dx, dh = layer.backward(dhs[:, t, :] + dh)\n",
    "            dxs[:, t, :] = dx\n",
    "            \n",
    "            for i, grad in enumerate(layer.grads):\n",
    "                grads[i] += grad\n",
    "                \n",
    "        for i, grad in enumerate(grads):\n",
    "            self.grads[i][...] = grad\n",
    "        self.dh = dh\n",
    "        \n",
    "        return dxs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chap. 8 Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.83034869 -0.0663667   0.70128839 -0.85694723]\n",
      " [ 0.07172487 -0.05830517 -0.0373014   0.09220855]\n",
      " [-0.03556187  0.01010812 -0.03337136 -0.07391818]\n",
      " [-0.04115198 -0.03137931  0.01341287  0.04080555]\n",
      " [ 0.01333651  0.03626031 -0.02652983  0.00663019]]\n",
      "====== SUM =====\n",
      "[ 0.83869621 -0.10968275  0.61749866 -0.79122112]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "T, H = 5, 4\n",
    "hs = np.random.randn(T, H)\n",
    "#print(hs)\n",
    "a = np.array([0.8, 0.1, 0.03, 0.05, 0.02])\n",
    "\n",
    "ar = a.reshape(5, 1).repeat(4, axis=1)\n",
    "#print(ar)\n",
    "\n",
    "t = hs*ar\n",
    "print(t)\n",
    "c = np.sum(t, axis=0)\n",
    "print(\"====== SUM =====\")\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
