{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Perceptron](https://en.wikipedia.org/wiki/Perceptron)\n",
    "\n",
    "## logic gates using perceptron\n",
    "\n",
    "XOR is expressed as AND( NAND(x1,x2), OR(x1,x2) ) and can be implemented as Multi Layer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AND\n",
    "def AND(x1, x2):\n",
    "    w1, w2, theta = 0.5, 0.5, 0.7\n",
    "    tmp = x1*w1 + x2*w2\n",
    "    if tmp >= theta:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def OR(x1, x2):\n",
    "    w1, w2, theta = 0.1, 0.1, 0.5\n",
    "    tmp = x1*w1 + x2*w2\n",
    "    if tmp==0:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "def NAND(x1, x2):\n",
    "    w1, w2, theta = 0.5, 0.5, 0.7\n",
    "    tmp = x1*w1 + x2*w2\n",
    "    if tmp < theta:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def XOR(x1, x2):\n",
    "    in1 = OR(x1,x2)\n",
    "    in2 = NAND(x1,x2)\n",
    "    return(AND(in1, in2))\n",
    "    \n",
    "# test\n",
    "print(\"AND\")\n",
    "print(AND(0, 0))\n",
    "print(AND(0, 1))\n",
    "print(AND(1, 0))\n",
    "print(AND(1, 1))\n",
    "\n",
    "print(\"OR\")\n",
    "print(OR(0, 0))\n",
    "print(OR(0, 1))\n",
    "print(OR(1, 0))\n",
    "print(OR(1, 1))\n",
    "\n",
    "print(\"NAND\")\n",
    "print(NAND(0, 0))\n",
    "print(NAND(0, 1))\n",
    "print(NAND(1, 0))\n",
    "print(NAND(1, 1))\n",
    "\n",
    "print(\"XOR\")\n",
    "print(XOR(0, 0))\n",
    "print(XOR(0, 1))\n",
    "print(XOR(1, 0))\n",
    "print(XOR(1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN\n",
    "\n",
    "* パーセプトロンの例では重みを（真理値表を踏まえて）人手で決めた。ニューラルネットワークを用いれば、適切な重みパラメータをデータから自動で学習できる。\n",
    "* パーセプトロンでは活性化関数にステップ関数を利用している。活性化関数をステップ関数から別の関数に変更する事でNNへ。\n",
    "* ステップ関数ではほとんどの場所（0以外の点）において傾きが0であるのに対して、シグモイド関数の傾きは0にならない。この性質がニューラルネットワークの学習にとって好ましい性質となっている。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of step function and sigmoid function\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "def step_function(x):\n",
    "    y = x>0\n",
    "    y = y.astype(np.int)\n",
    "    return y\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "# Step function\n",
    "x = np.arange(-5.0, 5.0, 0.1)\n",
    "y = step_function(x)\n",
    "plt.plot(x,y)\n",
    "plt.ylim(-0.1, 1.1)\n",
    "plt.show()\n",
    "\n",
    "# Sigmoid function\n",
    "x = np.arange(-5.0, 5.0, 0.1)\n",
    "y = sigmoid(x)\n",
    "plt.plot(x,y)\n",
    "plt.ylim(-0.1, 1.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Softmax 実装上の Tip\n",
    "\n",
    "定義式通りにコードを書くとオーバーフローが発生する可能性があるので、入力信号の最大の値を減算する事が一般的に行われる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# naive implementation\n",
    "a = np.array([1010, 1000, 990])\n",
    "np.exp(a) / np.sum(np.exp(a))\n",
    "# result:\n",
    "# array([nan, nan, nan])\n",
    "\n",
    "# modified implementation\n",
    "c = np.max(a)\n",
    "np.exp(a-c) / np.sum(np.exp(a-c))\n",
    "# result\n",
    "# array([9.99954600e-01, 4.53978686e-05, 2.06106005e-09])\n",
    "\n",
    "# softmax の出力の総和は 1.0 になる\n",
    "# つまり、確率として解釈する事が可能となっている\n",
    "sum(np.exp(a-c) / np.sum(np.exp(a-c)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST\n",
    "\n",
    "※なお、クラス分類では出力層のソフトマックスは省略可能。\n",
    "これは、exp(x)が単調増加する関数なので、ソフトマックスを使っても使わなくても出力の一番大きなニューロンの場所が変わらないため。\n",
    "実際に、指数関数の計算はコンピュータリソースが必要となるため出力層のソフトマックスは省略する事が多いようだ。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import pickle\n",
    "sys.path.append(\"./dlscratch\")\n",
    "from dataset.mnist import load_mnist\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from IPython import display\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def softmax(a):\n",
    "    c = np.max(a)\n",
    "    return (np.exp(a-c) / np.sum(np.exp(a-c)))\n",
    "\n",
    "def img_show(img):\n",
    "    #pil_img = Image.fromarray(np.uint8(img))\n",
    "    #pil_img.show()\n",
    "    plt.figure(3)\n",
    "    plt.clf()\n",
    "    plt.imshow(img)\n",
    "    #plt.title(\"%s | Step: %d %s\" % (env._spec.id,step, info))\n",
    "    plt.axis('off')\n",
    "\n",
    "    display.clear_output(wait=True)\n",
    "    display.display(plt.gcf())\n",
    "\n",
    "def get_data():\n",
    "    (x_train, t_train), (x_test, t_test) = load_mnist(flatten=True, normalize=False)\n",
    "    return x_train, t_train, x_test, t_test\n",
    "\n",
    "def init_network():\n",
    "    with open(\"./dlscratch/ch03/sample_weight.pkl\", 'rb') as f:\n",
    "        network = pickle.load(f)\n",
    "        return network\n",
    "    \n",
    "def predict(network, x):\n",
    "    W1, W2, W3 = network['W1'], network['W2'], network['W3']\n",
    "    b1, b2, b3 = network['b1'], network['b2'], network['b3']\n",
    "    \n",
    "    a1 = np.dot(x, W1) + b1\n",
    "    z1 = sigmoid(a1)\n",
    "    a2 = np.dot(z1, W2) + b2\n",
    "    z2 = sigmoid(a2)\n",
    "    a3 = np.dot(z2, W3) + b3\n",
    "    #print(a3.shape)\n",
    "    y = softmax(a3)\n",
    "    \n",
    "    return y\n",
    "    \n",
    "def main():\n",
    "    x_train, t_train, x_test, t_test = get_data()\n",
    "    network = init_network()\n",
    "    \n",
    "    img = x_train[0].reshape(28, 28)\n",
    "    img_show(img)\n",
    "    \n",
    "    accuracy_cnt = 0\n",
    "    for i in range(len(x_train)):\n",
    "        y = predict(network, x_train[i])\n",
    "        \n",
    "        if t_train[i] == np.argmax(y):\n",
    "            accuracy_cnt += 1\n",
    "            \n",
    "    print(\"Accuracy: {}\".format(accuracy_cnt / len(x_train)))\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch version\n",
    "import sys, os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import pickle\n",
    "sys.path.append(\"./dlscratch\")\n",
    "from dataset.mnist import load_mnist\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from IPython import display\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def softmax(a):\n",
    "    c = np.max(a)\n",
    "    return (np.exp(a-c) / np.sum(np.exp(a-c)))\n",
    "\n",
    "def img_show(img):\n",
    "    #pil_img = Image.fromarray(np.uint8(img))\n",
    "    #pil_img.show()\n",
    "    plt.figure(3)\n",
    "    plt.clf()\n",
    "    plt.imshow(img)\n",
    "    #plt.title(\"%s | Step: %d %s\" % (env._spec.id,step, info))\n",
    "    plt.axis('off')\n",
    "\n",
    "    display.clear_output(wait=True)\n",
    "    display.display(plt.gcf())\n",
    "\n",
    "def get_data():\n",
    "    (x_train, t_train), (x_test, t_test) = load_mnist(flatten=True, normalize=False)\n",
    "    return x_train, t_train, x_test, t_test\n",
    "\n",
    "def init_network():\n",
    "    with open(\"./dlscratch/ch03/sample_weight.pkl\", 'rb') as f:\n",
    "        network = pickle.load(f)\n",
    "        return network\n",
    "    \n",
    "def predict(network, x):\n",
    "    W1, W2, W3 = network['W1'], network['W2'], network['W3']\n",
    "    b1, b2, b3 = network['b1'], network['b2'], network['b3']\n",
    "    \n",
    "    a1 = np.dot(x, W1) + b1\n",
    "    z1 = sigmoid(a1)\n",
    "    a2 = np.dot(z1, W2) + b2\n",
    "    z2 = sigmoid(a2)\n",
    "    a3 = np.dot(z2, W3) + b3\n",
    "    #print(a3.shape)\n",
    "    y = softmax(a3)\n",
    "    \n",
    "    return y\n",
    "    \n",
    "def main():\n",
    "    x_train, t_train, x_test, t_test = get_data()\n",
    "    network = init_network()\n",
    "\n",
    "    batch_size = 100\n",
    "    accuracy_cnt = 0\n",
    "    \n",
    "    for i in range(0, len(x_train), batch_size):\n",
    "        x_batch = x_train[i:i+batch_size]\n",
    "        y = predict(network, x_batch)\n",
    "        \n",
    "        p = np.argmax(y, axis=1)\n",
    "        accuracy_cnt += np.sum(p == t_train[i:i+batch_size])\n",
    "            \n",
    "    print(\"Accuracy: {}\".format(accuracy_cnt / len(x_train)))\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning in NN\n",
    "\n",
    "「訓練データを使って学習する」＝訓練データに対する損失関数の値を求め、その値をできるだけ小さくするようなパラメータを探し出すこと\n",
    "\n",
    "### cross entropy loss\n",
    "\n",
    "$E = - \\sum_k{t_k log y_k}$\n",
    "※$k$はk次元目の値を意味する\n",
    "\n",
    "ここで、$y_k$はニューラルネットワークの出力、$t_k$は正解ラベルとなるインデックスだけが1でその他は0であるone-hotベクトル。そのため、実質的に上記の式は、正解ラベルが1に対応する出力の自然対数を計算するだけと言える。\n",
    "\n",
    "実装上は、$log(0)$がマイナス無限大になってしまうので以下のように微小な値を加えて対策をする点がtip。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def cross_entropy_error(y, t):\n",
    "    delta = 1e-7\n",
    "    return -np.sum(t * np.log(y+delta))\n",
    "\n",
    "t7 = [0, 0, 0, 0, 0, 0, 0, 1, 0, 0]\n",
    "t2 = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
    "y = [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0]\n",
    "print( cross_entropy_error(np.array(y), np.array(t2)) )\n",
    "print( cross_entropy_error(np.array(y), np.array(t7)) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_2(x):\n",
    "    # df/dx0 = 2*x0\n",
    "    # df/dx1 = 2*x1\n",
    "    return x[0]**2 + x[1]**2\n",
    "\n",
    "def numerical_gradient(f, x):\n",
    "    h = 1e-4  # 0.0001\n",
    "    grad = np.zeros_like(x)\n",
    "    \n",
    "    for idx in range(x.size):\n",
    "        tmp_val = x[idx]\n",
    "        \n",
    "        x[idx] = tmp_val + h\n",
    "        fxh1 = f(x)\n",
    "        \n",
    "        x[idx] = tmp_val - h\n",
    "        fxh2 = f(x)\n",
    "        \n",
    "        grad[idx] = (fxh1 - fxh2) / (2*h)\n",
    "        x[idx] = tmp_val\n",
    "        \n",
    "    return grad\n",
    "\n",
    "print( numerical_gradient(function_2, np.array([3.0, 4.0])) )\n",
    "print( numerical_gradient(function_2, np.array([0.0, 2.0])) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(f, init_x, lr=0.01, step_num=100):\n",
    "    x = init_x\n",
    "    \n",
    "    for i in range(step_num):\n",
    "        grad = numerical_gradient(f, x)\n",
    "        x -= lr*grad\n",
    "    \n",
    "    return x\n",
    "\n",
    "print(gradient_descent(function_2, np.array([3.0, 4.0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent in NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(\"./dlscratch\")\n",
    "import numpy as np\n",
    "from common.functions import softmax, cross_entropy_error\n",
    "from common.gradient import numerical_gradient\n",
    "\n",
    "class simpleNet:\n",
    "    def __init__(self):\n",
    "        self.W = np.random.randn(2, 3)\n",
    "        \n",
    "    def predict(self, x):\n",
    "        return np.dot(x, self.W)\n",
    "    \n",
    "    def loss(self, x, t):\n",
    "        z = self.predict(x)\n",
    "        y = softmax(z)\n",
    "        loss = cross_entropy_error(y, t)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "net = simpleNet()\n",
    "print(net.W)\n",
    "x = np.array([0.6, 0.9])\n",
    "p = net.predict(x)\n",
    "print(p)\n",
    "np.argmax(p)\n",
    "t = np.array([0, 0, 1])\n",
    "net.loss(x, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(W):\n",
    "    return net.loss(x, t)\n",
    "\n",
    "dW = numerical_gradient(f, net.W)\n",
    "print(dW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SGD\n",
    "\n",
    "ニューラルネットワークの学習は勾配降下法によってパラメータを更新する方法だが、ここで使用するデータはミニバッチとして無作為に選ばれたデータを使用している事から、確率敵勾配降下法（Stochastic Gradient Descent）と呼ばれる。「確率的」の理由はここにある。つまり、「確率的に無作為に選び出した」という意味。\n",
    "\n",
    "### Two Layers NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(\"./dlscratch\")\n",
    "import numpy as np\n",
    "from common.functions import softmax, cross_entropy_error, sigmoid\n",
    "from common.gradient import numerical_gradient\n",
    "\n",
    "class TwoLayerNet:\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):\n",
    "        # initialize weight\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "        \n",
    "    def predict(self, x):\n",
    "        W1, W2 = self.params['W1'], self.params['W2']\n",
    "        b1, b2 = self.params['b1'], self.params['b2']\n",
    "        \n",
    "        a1 = np.dot(x, W1) + b1\n",
    "        z1 = sigmoid(a1)\n",
    "        a2 = np.dot(z1, W2) + b2\n",
    "        y = softmax(a2)\n",
    "        \n",
    "        return y\n",
    "    \n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        \n",
    "        return cross_entropy_error(y, t)\n",
    "    \n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        t = np.argmax(t, axis=1)\n",
    "        \n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "    \n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "        \n",
    "        grads = {}\n",
    "        \n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "        \n",
    "        return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from dataset.mnist import load_mnist\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "train_loss_list = []\n",
    "\n",
    "iters_num = 10000\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "learning_rate = 0.1\n",
    "\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "print(\"start training\")\n",
    "x = np.linspace(0, 1, iters_num)\n",
    "for i in range(iters_num):\n",
    "    # get a mini batch\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "    \n",
    "    # compute gradient\n",
    "    grad = network.numerical_gradient(x_batch, t_batch)\n",
    "    \n",
    "    # update parameters\n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "        \n",
    "    # record learning\n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)\n",
    "    print(loss)\n",
    "\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.plot(train_loss_list, color='lightblue', linewidth=3)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'MulLayer' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-5630ea643cb8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;31m# forward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0mapple_price\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmul_apple_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapple\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapple_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m \u001b[0mprice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmul_tax_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapple_price\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtax\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'MulLayer' object is not callable"
     ]
    }
   ],
   "source": [
    "class MulLayer:\n",
    "    def __init__(self):\n",
    "        self.x = None\n",
    "        self.y = None\n",
    "        \n",
    "    def forward(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        out = x * y\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dx = dout * self.y\n",
    "        dy = dout * self.x\n",
    "        \n",
    "        return dx, dy\n",
    "    \n",
    "\n",
    "class AddLayer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def forward(self, x, y):\n",
    "        out = x + y\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dx = dout * 1\n",
    "        dy = dout * 1\n",
    "        return dx, dy\n",
    "    \n",
    "# example\n",
    "apple = 100\n",
    "apple_num = 2\n",
    "tax = 1.1\n",
    "\n",
    "mul_apple_layer = MulLayer()\n",
    "mul_tax_layer = MulLayer()\n",
    "\n",
    "# forward\n",
    "apple_price = mul_apple_layer.forward(apple, apple_num)\n",
    "price = mul_tax_layer(apple_price, tax)\n",
    "\n",
    "print(price)\n",
    "\n",
    "# backward\n",
    "dprice = 1\n",
    "dapple_price, dtax = mul_tax_layer.backward(dprice)\n",
    "dapple, dapple_num = mul_apple_layer.backward(dapple_price)\n",
    "\n",
    "print(dapple, dapple_num, dtax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation Functions Layer\n",
    "\n",
    "* ReLU\n",
    "* Sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[False  True]\n",
      " [ True False]]\n",
      "[[1. 0.]\n",
      " [0. 3.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Relu:\n",
    "    def __init__(self):\n",
    "        self.mask = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.mask = (x <= 0)  # 便利なので覚えておきたい使い方\n",
    "        out = x.copy()\n",
    "        out[self.mask] = 0\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dout[self.mask] = 0\n",
    "        dx = dout\n",
    "        \n",
    "        return dx\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
