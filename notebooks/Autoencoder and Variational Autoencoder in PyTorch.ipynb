{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoder\n",
    "\n",
    "[Reducing the Dimensionality of Data with Neural Networks](http://science.sciencemag.org/content/313/5786/504)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.utils import save_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda = torch.cuda.is_available()\n",
    "\n",
    "### in case GPU is used for the other experiments now...\n",
    "#cuda = False\n",
    "###\n",
    "\n",
    "if cuda:\n",
    "    print(\"CUDA is available\")\n",
    "\n",
    "num_epochs = 100\n",
    "batch_size = 128\n",
    "learning_rate = 0.001\n",
    "\n",
    "out_dir = './results/autoencoder'\n",
    "\n",
    "if not os.path.exists(out_dir):\n",
    "    print(\"No out_dir exists. Create one.\")\n",
    "    os.mkdir(out_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "train_dataset = MNIST('./data', download=True, transform=img_transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(28 * 28, 128),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(64, 12),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(12, 2)\n",
    "        )\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(2, 12),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(12, 64),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(128, 28 * 28),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Autoencoder()\n",
    "if cuda:\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_image(x):\n",
    "    x = 0.5 * (x + 1)  # [-1, 1] => [0, 1]\n",
    "    x = x.clamp(0, 1)\n",
    "    x = x.view(x.size(0), 1, 28, 28)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(),\n",
    "                            lr=learning_rate,\n",
    "                            weight_decay=1e-5)\n",
    "\n",
    "loss_list = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for data in train_loader:\n",
    "        img, _ = data\n",
    "        #print(\"original: {}\".format(img.size(0)))\n",
    "        x = img.view(img.size(0), -1)\n",
    "        #print(\"reshaped: {}\".format(x.size()))\n",
    "        if cuda:\n",
    "            x = Variable(x).cuda()\n",
    "        else:\n",
    "            x = Variable(x)\n",
    "            \n",
    "        xhat = model(x)\n",
    "        \n",
    "        loss = criterion(xhat, x)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # logging\n",
    "        loss_list.append(loss.data[0])\n",
    "        \n",
    "    print('epoch [{}/{}], loss: {:.4f}'.format(\n",
    "        epoch + 1,\n",
    "        num_epochs,\n",
    "        loss.data[0]\n",
    "    ))\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        pic = to_image(xhat.cpu().data)\n",
    "        save_image(pic, '{}/image_{}.png'.format(out_dir, epoch))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_list = np.load('{}/loss_list.npy'.format(out_dir))\n",
    "\n",
    "import pylab\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.plot(loss_list)\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('loss')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image('results/autoencoder/image_0.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('{}/autoencoder.pt'.format(out_dir),\n",
    "                                map_location=lambda storage,\n",
    "                                loc: storage))\n",
    "\n",
    "test_dataset = MNIST('./data', download=True, train=False, transform=img_transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=10000, shuffle=False)\n",
    "\n",
    "images, labels = iter(test_loader).next()\n",
    "images = images.view(10000, -1)\n",
    "print(images.shape)\n",
    "\n",
    "z = model.encoder(Variable(images.cuda(), volatile=True)).cpu()\n",
    "z.data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Need to fix\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.scatter(z[:,0], z[:,1], marker='.', c=labels.numpy(), cmap=pylab.cm.jet)\n",
    "plt.colorbar()\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test on Montezuma's image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.utils import save_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv_Autoencoder(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Conv_Autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=16, kernel_size=8, stride=4),\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=5, stride=2),\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(2, stride=1)\n",
    "            #nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1),\n",
    "            #nn.ReLU(True)\n",
    "        )\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            # Deconvolution = Transoised Convolution\n",
    "            # https://pytorch.org/docs/stable/nn.html#torch.nn.ConvTranspose2d\n",
    "            \n",
    "            # Implementation 1\n",
    "            # Some random parameters to forcefully work this guy anyways...\n",
    "            nn.ConvTranspose2d(in_channels=32, out_channels=16, kernel_size=5, stride=3, output_padding=2),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(in_channels=16, out_channels=8, kernel_size=5, stride=3, output_padding=(0,2)),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(in_channels=8, out_channels=3, kernel_size=3, stride=2, output_padding=(1,1)),\n",
    "            nn.Tanh()\n",
    "            \n",
    "            # Need to fix. See https://arxiv.org/abs/1603.07285\n",
    "            # 4.6 Zero padding, non-unit strides, transposed\n",
    "            #nn.ConvTranspose2d(in_channels=32, out_channels=16, kernel_size=5, stride=3, output_padding=(0,2)),\n",
    "            #nn.ReLU(True),\n",
    "            #nn.ConvTranspose2d(in_channels=16, out_channels=3, kernel_size=3, stride=2, output_padding=(1,1)),\n",
    "            #nn.Tanh()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-09-12 17:21:54,907] Making new env: MontezumaRevenge-v0\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make('MontezumaRevenge-v0').unwrapped\n",
    "\n",
    "NUM_DATASET = 1000\n",
    "s = env.reset()\n",
    "memory = []\n",
    "\n",
    "# collecting the dataset as input for Autoencoder\n",
    "for i in range(NUM_DATASET):\n",
    "    action = env.action_space.sample()\n",
    "    s, r, done, info = env.step(action)\n",
    "    memory.append(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recon_image(img):\n",
    "    \"\"\"\n",
    "    expected input img: torch.Size([1, 3, 210, 160])\n",
    "             output   : ndarray.Size([210, 160, 3])\n",
    "             => squeez 0th dimention and np.transpose(img, (1, 2, 0))\n",
    "    \"\"\"\n",
    "    img = img.cpu()\n",
    "    img = torch.squeeze(img, 0)\n",
    "    img = img.detach().numpy()\n",
    "    img = np.transpose(img, (1, 2, 0))\n",
    "    \n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/takashi/anaconda3/envs/cs294/lib/python3.5/site-packages/ipykernel_launcher.py:53: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "cuda = torch.cuda.is_available()\n",
    "num_epochs = 100\n",
    "batch_size = 128\n",
    "learning_rate = 0.001\n",
    "\n",
    "model = Conv_Autoencoder()\n",
    "if cuda:\n",
    "    print(\"CUDA is available\")\n",
    "    model.cuda()\n",
    "\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(),\n",
    "                            lr=learning_rate,\n",
    "                            weight_decay=1e-5)\n",
    "\n",
    "loss_list = []\n",
    "\n",
    "out_dir = './results/autoencoder'\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for img in memory:\n",
    "        # img shape is (210, 160, 3)\n",
    "        #plt.imshow(img)\n",
    "        img = np.transpose(img, (2, 0, 1))\n",
    "        #print(\"reshaped: {}\".format(x.size()))\n",
    "        img = torch.from_numpy(img)\n",
    "        img.unsqueeze_(0)  # https://discuss.pytorch.org/t/expected-stride-to-be-a-single-integer-value-or-a-list/17612\n",
    "        if cuda:\n",
    "            img = Variable(img).cuda()\n",
    "            img = img.type(torch.cuda.FloatTensor)\n",
    "        else:\n",
    "            img = Variable(img)\n",
    "            img = img.type(torch.FloatTensor)\n",
    "        \n",
    "        #img.float()\n",
    "        xhat = model(img)\n",
    "        \n",
    "        # debug imshow\n",
    "        hat = recon_image(xhat)\n",
    "        cv2.imshow('test', hat)\n",
    "        cv2.waitKey(0)\n",
    "        \n",
    "        loss = criterion(xhat, img/255)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # logging\n",
    "        loss_list.append(loss.data[0])\n",
    "        \n",
    "    print('epoch [{}/{}], loss: {:.4f}'.format(\n",
    "        epoch + 1,\n",
    "        num_epochs,\n",
    "        loss.data[0]\n",
    "    ))\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        save_image(torch.squeeze(xhat, 0), '{}/image_{}.png'.format(out_dir, epoch))\n",
    "        \n",
    "np.save('{}/loss_list.npy'.format(out_dir), np.array(loss_list))\n",
    "torch.save(model.state_dict(), '{}/mz_autoencoder.pt'.format(out_dir))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = env.reset()\n",
    "img = np.transpose(img, (2, 0, 1))\n",
    "img = torch.from_numpy(img)\n",
    "img.unsqueeze_(0)  # https://discuss.pytorch.org/t/expected-stride-to-be-a-single-integer-value-or-a-list/17612\n",
    "if cuda:\n",
    "    img = Variable(img).cuda()\n",
    "    img = img.type(torch.cuda.FloatTensor)\n",
    "else:\n",
    "    img = Variable(img)\n",
    "    img = img.type(torch.FloatTensor)\n",
    "\n",
    "z = model.encoder(img).cpu()\n",
    "z = torch.squeeze(z, 0)\n",
    "z.data.numpy()\n",
    "print(z.shape)\n",
    "print(\"Original is 210*160*3 = {}, Latent space rep is 32*10*7 = {}\".format(210*160*3, 32*10*7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational Autoencoder\n",
    "\n",
    "You can find a sample implementation in [pytorch/examples](https://github.com/pytorch/examples/blob/master/vae/main.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
