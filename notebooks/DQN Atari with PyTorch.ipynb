{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import gym.spaces\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "import torchvision.datasets as dsets\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# Below are just for debugging\n",
    "import pickle\n",
    "import os\n",
    "import sys\n",
    "import gc\n",
    "import psutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-08-31 16:51:33,292] Making new env: SpaceInvaders-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box(210, 160, 3)\n",
      "Discrete(6)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('SpaceInvaders-v0').unwrapped\n",
    "#env = gym.make('MontezumaRevenge-v0').unwrapped\n",
    "print(env.observation_space)\n",
    "print(env.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "# if gpu is to be used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Saves a transition.\"\"\"\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output formula\n",
    "$O = \\frac{W-K+2P}{S}+1$\n",
    "    - O: Output height/width\n",
    "    - W: Input height/length\n",
    "    - K: filter size (kernel size)\n",
    "    - P: Padding\n",
    "    - S: Stride\n",
    "    \n",
    "<!--\n",
    "conv1:    3 x (206, 156)\n",
    "maxpool1: 3 x (103, 78)\n",
    "conv2: 3 x (99, 74)\n",
    "maxpool2: 3 x (49, 37)\n",
    "conv3: 3 x (45, 33)\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(DQN, self).__init__()\n",
    "        #self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=5, stride=1)\n",
    "        #self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=8, stride=4)\n",
    "        #self.bn1 = nn.BatchNorm2d(32)\n",
    "        \n",
    "        #self.maxpool1 = nn.MaxPool2d(kernel_size=2)\n",
    "        \n",
    "        #self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=5, stride=1)\n",
    "        #self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2)\n",
    "        #self.bn2 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        #self.maxpool2 = nn.MaxPool2d(kernel_size=2)\n",
    "            \n",
    "        #self.conv3 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=5, stride=1)\n",
    "        #self.bn3 = nn.BatchNorm2d(32)\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1)\n",
    "        #self.bn3 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        #self.head = nn.Linear(47520, 6)\n",
    "        #self.head = nn.Linear(34848, 6)  # with maxpool1 and 2\n",
    "        #self.fc = nn.Linear(700928, 6)\n",
    "        #self.fc = nn.Linear(16384, 6)\n",
    "        self.fc1 = nn.Linear(in_features=16384, out_features=512)\n",
    "        self.fc2 = nn.Linear(in_features=512, out_features=6)  # out_features should be parametrized to env.action_space\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv1(x))\n",
    "        #x = self.maxpool1(x)\n",
    "        x = self.relu(self.conv2(x))\n",
    "        #x = self.maxpool2(x)\n",
    "        x = self.relu(self.conv3(x))\n",
    "        x = self.fc1(x.view(x.size(0), -1))\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "resize = T.Compose([T.ToPILImage(),\n",
    "                    T.Resize(40, interpolation=Image.CUBIC),\n",
    "                    T.ToTensor()])\n",
    "\n",
    "def get_screen():\n",
    "    screen = env.render(mode='rgb_array').transpose(\n",
    "        (2, 0, 1))\n",
    "    #screen = env.render(mode='rgb_array')\n",
    "    screen = np.ascontiguousarray(screen, dtype=np.float32)\n",
    "    screen = torch.from_numpy(screen)\n",
    "    #print(\"The shape of original screen is {}\".format(screen.size()))\n",
    "    #screen = screen[30:200,:]\n",
    "    #return resize(screen).unsqueeze(0).to(device)\n",
    "    return screen.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "GAMMA = 0.99\n",
    "EPS_START = 1.0\n",
    "EPS_END = 0.1\n",
    "EPS_DECAY = 1000000\n",
    "TARGET_UPDATE = 10000\n",
    "LR = 0.00025\n",
    "\n",
    "policy_net = DQN().to(device)\n",
    "target_net = DQN().to(device)\n",
    "policy_net.load_state_dict(torch.load('invaders_policy.pt'))\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "\n",
    "optimizer = optim.RMSprop(policy_net.parameters(), lr=LR)\n",
    "memory = ReplayMemory(4500)  # Original paper used 1000000...\n",
    "\n",
    "steps_done = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_action(state):\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * math.exp(-1. * steps_done / EPS_DECAY)\n",
    "    #print(\"eps_threshold is {}\".format(eps_threshold))\n",
    "    steps_done += 1\n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            #a = policy_net(state)\n",
    "            # a looks like this: tensor([[ 0.5946,  0.0787,  0.4172,  0.4470, -0.3384,  0.0064]], device='cuda:0')\n",
    "            return policy_net(state).max(1)[1]\n",
    "    else:\n",
    "        return torch.tensor([[random.randrange(6)]], device=device, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    # Transpose the batch (see http://stackoverflow.com/a/19343/3343043 for\n",
    "    # detailed explanation).\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # Compute a mask of non-final states and concatenate the batch elements\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                          batch.next_state)), device=device, dtype=torch.uint8)\n",
    "\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                                if s is not None])\n",
    "\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.tensor(batch.action, device=device)\n",
    "    action_batch = action_batch.view(BATCH_SIZE,-1)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "    # columns of actions taken\n",
    "    Q_s_a = policy_net(state_batch)\n",
    "\n",
    "    actions = Q_s_a.gather(1, action_batch)\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "    # Compute V(s_{t+1}) for all next states.\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    \n",
    "    #non_final_next_states = non_final_next_states.view(BATCH_SIZE, 3, 210, 160)\n",
    "    non_final_next_states = non_final_next_states.view(BATCH_SIZE, 3, 160, 160)\n",
    "    target = target_net(non_final_next_states)\n",
    "    next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0].detach()\n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    # Compute Huber loss\n",
    "    loss = F.smooth_l1_loss(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    for param in policy_net.parameters():\n",
    "        param.grad.data.clamp_(-1, 1)\n",
    "    optimizer.step()\n",
    "    \n",
    "    \n",
    "    # For debugging\n",
    "    #cpuStats()\n",
    "    #memReport()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "s = get_screen()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_screen(s):\n",
    "    ### trim a screen a bit\n",
    "    s = s[:,35:195,:]\n",
    "    return s\n",
    "\n",
    "def memReport():\n",
    "    for obj in gc.get_objects():\n",
    "        if torch.is_tensor(obj) or (hasattr(obj, 'data') and torch.is_tensor(obj.data)):\n",
    "            print(type(obj), obj.size())\n",
    "    \n",
    "def cpuStats():\n",
    "        print(sys.version)\n",
    "        print(psutil.cpu_percent())\n",
    "        print(psutil.virtual_memory())\n",
    "        pid = os.getpid()\n",
    "        py = psutil.Process(pid)\n",
    "        memoryUse = py.memory_info()[0] / 2. ** 30\n",
    "        print('memory GB:', memoryUse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 epochs has finished\n",
      "Got reward tensor([ 120.], device='cuda:0')\n",
      "10 epochs has finished\n",
      "Got reward tensor([ 105.], device='cuda:0')\n",
      "20 epochs has finished\n",
      "Got reward tensor([ 140.], device='cuda:0')\n",
      "30 epochs has finished\n",
      "Got reward tensor([ 210.], device='cuda:0')\n",
      "40 epochs has finished\n",
      "Got reward tensor([ 440.], device='cuda:0')\n",
      "50 epochs has finished\n",
      "Got reward tensor([ 110.], device='cuda:0')\n",
      "60 epochs has finished\n",
      "Got reward tensor([ 90.], device='cuda:0')\n",
      "70 epochs has finished\n",
      "Got reward tensor([ 155.], device='cuda:0')\n",
      "80 epochs has finished\n",
      "Got reward tensor([ 30.], device='cuda:0')\n",
      "90 epochs has finished\n",
      "Got reward tensor([ 545.], device='cuda:0')\n",
      "100 epochs has finished\n",
      "Got reward tensor([ 100.], device='cuda:0')\n",
      "110 epochs has finished\n",
      "Got reward tensor([ 105.], device='cuda:0')\n",
      "120 epochs has finished\n",
      "Got reward tensor([ 90.], device='cuda:0')\n",
      "130 epochs has finished\n",
      "Got reward tensor([ 210.], device='cuda:0')\n",
      "140 epochs has finished\n",
      "Got reward tensor([ 250.], device='cuda:0')\n",
      "150 epochs has finished\n",
      "Got reward tensor([ 140.], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 200\n",
    "\n",
    "for i in range(NUM_EPOCHS):\n",
    "    env.reset()\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    while not done:\n",
    "        state = get_screen()\n",
    "        state = trim_screen(state)\n",
    "        action = get_action(state.unsqueeze_(0))\n",
    "        env.render()\n",
    "        _, reward, done, info =  env.step(action.item())\n",
    "        reward = torch.tensor([reward], device=device)\n",
    "        total_reward += reward\n",
    "        next_state = get_screen()\n",
    "        next_state = trim_screen(next_state)\n",
    "        #print(\"S:{} A:{} S':{} R:{}\".format(state, action.item(), next_state, reward))\n",
    "        memory.push(state, action.item(), next_state, reward)\n",
    "        \n",
    "        optimize_model()\n",
    "      \n",
    "    #steps_done = 0\n",
    "    if (i % 10 == 0):\n",
    "        print(\"{} epochs has finished\".format(i))\n",
    "        print(\"Got reward {}\".format(total_reward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(policy_net.state_dict(), \"./invaders_policy.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just a test of trimming\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import numpy as np\n",
    "s = get_screen()\n",
    "s = s[:,35:195,:]\n",
    "print(s.shape)\n",
    "s = s.cpu()\n",
    "res = T.ToPILImage()(s)\n",
    "res.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
